{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42fa80f-432b-40d7-b0c0-63b22045374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ------------------------- Webscrapping Assignment ------------------------\n",
    "Que 1 - What is web scraping? Why is it used? Give three area where web scrapping is used to get data.\n",
    "Ans - Web Scraping - . Web scraping is the automated process of collecting unstructured data and representing in a\n",
    "structured form. Web scraping is a technique used to collect content and data from the internet. This data is \n",
    "stored on a local machine for manipulation and analysis at a later stage. It refers to the extraction of data from\n",
    "a web site. Web scraping is also called web harvesting or web data extraction. \n",
    "\n",
    "Why is it used - Web scraping is used to extract huge amount of data from internet in a shorter time for analysis\n",
    "and other purposes.\n",
    "\n",
    "Areas of Applications: - \n",
    "a) Collecting data from Shopping sites like Amazon, Flipkart to support competitor analysis\n",
    "b) Flight listings\n",
    "c) Collecting data for research and analysis\n",
    "d) Collecting data from social media platforms for customer sentiment analysis\n",
    "e) getting real estate listings\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d8837-25e0-41b4-bd66-354a7416777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Que 2 - What are the different methods used for web scraping?\n",
    "Ans - Various methods used for web scraping are as follows -\n",
    "a) Web Scraping libraries. Various web scraping libraries like Beautiful Soup, Scrapy, Puppeteer, Cheerio and\n",
    "Selenium are used for web scraping.\n",
    "b) Web Scraping tools - Web Scraping tools are softwares or programs that automatically gathers data from web\n",
    "sources.  \n",
    "c) Web Scraping APIs - Web scraping APIs enable developers to access and extract relevant data from websites.\n",
    "Websites can provide web scraping APIs, such as Twitter API, Amazon API, and Facebook API.\n",
    "d) Use of Optical Character Recognition (OCR) - It is a technology that allows users to extract text data from\n",
    "images (screen scraping) or scanned documents on web pages.\n",
    "e) Headless Browsers - Headless browsers enable users to collect web data in a headless mode, meaning that it \n",
    "runs without a graphical user interface. Headless browsers can be a powerful tool for scraping dynamic and\n",
    "interactive websites that employ client-side or server-side scripting. Web crawlers can access and extract data\n",
    "that may not be visible in the HTML code using headless browsers.\n",
    "e.g. of headless browsers are PhantomJS, Puppeteer, or Selenium .\n",
    "f) Use of HTML Parsing - HTML parsing is another technique used to automatically extract data from HTML code.\n",
    "g) DOM Parsing - DOM parsing allows you to parse HTML or XML documents into their corresponding Document Object \n",
    "Model (DOM) representation. DOM Parser is part of the W3C standard that provides methods to navigate the DOM \n",
    "tree and extract desired information from it, such as text or attributes.\n",
    "h) Manual Navigation - It is the process of manually navigating through a website and collecting web data along\n",
    "the way.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187940c-8db5-491e-9050-47eba5f24da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Que 3 - What is Beautiful Soup? Why is it used?\n",
    "Ans - Beautiful Soup - Beautiful Soup is a python package and as the name suggests, parses the unwanted data and\n",
    "helps to organize and format the messy web data by fixing bad HTML and present to us in an easily-traversible \n",
    "XML structures. In short, Beautiful Soup is a python package which allows us to pull data out of HTML and XML \n",
    "documents.\n",
    "Three features make it powerful:\n",
    "a) Beautiful Soup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a \n",
    "parse tree: a toolkit for dissecting a document and extracting what you need. It doesn't take much code to write\n",
    "an application.\n",
    "b) Beautiful Soup automatically converts incoming documents to Unicode and outgoing documents to UTF-8. You \n",
    "don't have to think about encodings, unless the document doesn't specify an encoding and Beautiful Soup can't \n",
    "detect one. Then you just have to specify the original encoding.\n",
    "c) Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out \n",
    "different parsing strategies or trade speed for flexibility.\n",
    "\n",
    "Use - Beautiful Soup is a Python library that is used for web scraping. It allows us to parse HTML and XML \n",
    "documents and extract the needed data. It is commonly used for data mining and data analysis, as well as \n",
    "automating tasks such as filling out forms or logging into websites.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4b34a-556e-48a8-9f3a-011f2b393025",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Que 4 - Why is flask used in web scraping project?\n",
    "Ans -\n",
    "a) Flask is used in web scraping projects because it does not rely on external libraries to perform framework\n",
    "tasks.  \n",
    "b) Flask is more adaptable to different working styles and approaches. Flask is preferred by programmers with \n",
    "more coding experience or who require more control over the app design.\n",
    "c) Flask supports multiple types of databases by default because it has no default model. This also simplifies \n",
    "the integration of databases into Flask applications.\n",
    "d) Flask makes life easier for creating a simple web app with a few static pages. Many programmers find Flask \n",
    "to be easily scalable for smaller web applications.\n",
    "e) It comes with a built-in development server and fast debugger.\n",
    "f) Flask is more independent and flexible.\n",
    "g) It has its own set of tools, technologies, and libraries.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93da11d-3608-4b89-9c89-e231190a05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Que 5 - Write the names of AWS services used in this project. Also expalain the use of each service.\n",
    "Ans - AWS services used in the project web scraping are as follows: -\n",
    "a) Code Pipeline - AWS CodePipeline is a continuous delivery service used to model, visualize and automate the \n",
    "steps required to release the software. With AWS CodePipeline, we can model the full release process for \n",
    "building code, deploying to pre-production environments, testing application and releasing it to production. AWS\n",
    "CodePipeline then builds, tests, and deploys application according to the defined workflow every time there is \n",
    "a code change. We can integrate partner tools and our own custom tools into any stage of the release process to\n",
    "form an end-to-end continuous delivery solution.\n",
    "\n",
    "\n",
    "b) AWS Elastic BeanStalk - AWS Elastic Beanstalk is a service using which we can quickly deploy and manage \n",
    "applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. \n",
    "Elastic Beanstalk reduces management complexity without restricting choice or control. We simply upload our \n",
    "application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing,\n",
    "scaling, and application health monitoring.\n",
    "AWS Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby. When \n",
    "we deploy our application, AWS Elastic Beanstalk builds the selected supported platform version and provisions\n",
    "one or more AWS resources, such as Amazon EC2 instances, to run our application.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
